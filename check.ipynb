{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PUS_teIrjIT",
        "outputId": "8861c38a-97ca-4285-d0dd-125d7e7b8c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Collecting streamlit-folium\n",
            "  Downloading streamlit_folium-0.25.0-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.4.0)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.37.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit_folium-0.25.0-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.4/328.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pypdfium2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, pydeck, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pdfminer.six, nvidia-cusolver-cu12, pdfplumber, streamlit, streamlit-folium\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pdfminer.six-20250327 pdfplumber-0.11.6 pydeck-0.9.1 pypdfium2-4.30.1 streamlit-1.45.0 streamlit-folium-0.25.0 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch pandas streamlit folium streamlit-folium plotly pdfplumber requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8vEHNZLrwHr",
        "outputId": "040faf58-3290-4fac-d0ab-c57a1e90dfdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app3.py &>/dev/null &\n",
        "\n",
        "\n",
        "# Expose the Streamlit app using ngrok\n",
        "!ssh -o StrictHostKeyChecking=no -R 80:localhost:8501 serveo.net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCNWBVuurw_U",
        "outputId": "0a9c2465-aa78-4d92-c845-18633ba046cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh: connect to host serveo.net port 22: Connection refused\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Streamlit and ngrok (already done, but included for completeness)\n",
        "!pip install streamlit\n",
        "!pip install ngrok\n",
        "!pip install pyngrok\n",
        "\n",
        "# Run Streamlit app in the background (already running)\n",
        "!streamlit run app3.py &>/dev/null &\n",
        "\n",
        "# Set up ngrok with pyngrok\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# Set auth token\n",
        "conf.get_default().auth_token = \"2wavhDIIlhw5QwDrujdyrq3q1y9_7DPqWdnGKtAXKQFUTPuta\"\n",
        "\n",
        "# Terminate existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a new tunnel to port 8501\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(\"Public URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5LkGgBit34_",
        "outputId": "0a48badb-fbfc-4055-cd64-c4c2c7d787c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.37.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: ngrok in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.7)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Public URL: NgrokTunnel: \"https://9373-34-142-196-61.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fJYZ2BouN8f",
        "outputId": "bd1f1b4f-de6c-470d-c59c-483bf9e634cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "streamlit 2067 root    6u  IPv4  67492      0t0  TCP *:8501 (LISTEN)\n",
            "streamlit 2067 root    7u  IPv6  67493      0t0  TCP *:8501 (LISTEN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2wavhDIIlhw5QwDrujdyrq3q1y9_7DPqWdnGKtAXKQFUTPuta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Xz2yb9u_DD",
        "outputId": "45aee614-8cd6-4e04-a4e8-36b14183209e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ngrok: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app3.py &>/dev/null &"
      ],
      "metadata": {
        "id": "KqDPgEHHs6rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app3.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import folium\n",
        "from streamlit_folium import folium_static\n",
        "import pdfplumber\n",
        "import requests\n",
        "import torch\n",
        "import time\n",
        "import threading\n",
        "import hashlib\n",
        "import concurrent.futures\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# News API setup\n",
        "NEWS_API_KEY = '0c41e6bff83444978d43b7adf46540ed'\n",
        "BASE_URL = 'https://newsapi.org/v2/everything'\n",
        "\n",
        "# Cache for responses\n",
        "response_cache = {}\n",
        "\n",
        "# Initialize session state variables\n",
        "if 'model' not in st.session_state:\n",
        "    st.session_state.model = None\n",
        "if 'tokenizer' not in st.session_state:\n",
        "    st.session_state.tokenizer = None\n",
        "if 'device' not in st.session_state:\n",
        "    st.session_state.device = None\n",
        "if 'model_loaded' not in st.session_state:\n",
        "    st.session_state.model_loaded = False\n",
        "if 'advanced_mode' not in st.session_state:\n",
        "    st.session_state.advanced_mode = False\n",
        "\n",
        "# Check for GPU availability with detailed info\n",
        "def check_gpu_availability():\n",
        "    \"\"\"Check GPU availability and return detailed information\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        # Get GPU details\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n",
        "\n",
        "        st.sidebar.success(f\"✅ GPU acceleration available: {gpu_name}\")\n",
        "        st.sidebar.info(f\"Total VRAM: {total_memory:.2f} GB\")\n",
        "\n",
        "        # Get current memory usage\n",
        "        if hasattr(torch.cuda, 'memory_allocated'):\n",
        "            allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "            st.sidebar.info(f\"VRAM Used: {allocated:.2f} GB\")\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        cpu_count = concurrent.futures.ThreadPoolExecutor()._max_workers\n",
        "        st.sidebar.info(f\"ℹ️ Running on CPU ({cpu_count} cores)\")\n",
        "\n",
        "    return device\n",
        "\n",
        "# Model loading with advanced options\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def load_advanced_model(device, advanced_mode=False):\n",
        "    \"\"\"Load appropriate model based on hardware and settings\"\"\"\n",
        "    try:\n",
        "        # Determine model based on hardware capabilities and settings\n",
        "        if advanced_mode and device == \"cuda\":\n",
        "            # Check VRAM for appropriate model selection\n",
        "            try:\n",
        "                vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "                if vram_gb >= 16:\n",
        "                    # Large model for high-end hardware\n",
        "                    model_name = \"NousResearch/Nous-Hermes-2-Yi-9B\"\n",
        "                    st.sidebar.success(f\"Loading advanced 9B model ({model_name})\")\n",
        "                elif vram_gb >= 8:\n",
        "                    # Medium model for mid-range hardware\n",
        "                    model_name = \"Qwen/Qwen1.5-7B-Chat\"\n",
        "                    st.sidebar.success(f\"Loading mid-range 7B model ({model_name})\")\n",
        "                else:\n",
        "                    # Smaller model for limited hardware\n",
        "                    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "                    st.sidebar.success(f\"Loading compact 1.1B model ({model_name})\")\n",
        "            except Exception as e:\n",
        "                # Fallback if VRAM detection fails\n",
        "                model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "                st.sidebar.warning(f\"VRAM detection failed, using compact model: {e}\")\n",
        "        else:\n",
        "            # Standard mode - use reliable small model\n",
        "            model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "            st.sidebar.info(f\"Loading standard model: {model_name}\")\n",
        "\n",
        "        # Load tokenizer first\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # Configure loading options based on device and available memory\n",
        "        load_kwargs = {\n",
        "            \"device_map\": \"auto\",\n",
        "            \"low_cpu_mem_usage\": True\n",
        "        }\n",
        "\n",
        "        # Add GPU optimizations if available\n",
        "        if device == \"cuda\":\n",
        "            try:\n",
        "                vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "                if vram_gb >= 16:\n",
        "                    # Use FP16 for larger GPUs\n",
        "                    load_kwargs[\"torch_dtype\"] = torch.float16\n",
        "                    # Use flash attention if supported\n",
        "                    if model_name in [\"NousResearch/Nous-Hermes-2-Yi-9B\", \"Qwen/Qwen1.5-7B-Chat\"]:\n",
        "                        load_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
        "                elif vram_gb >= 8:\n",
        "                    # Use 8-bit quantization for mid-range GPUs\n",
        "                    load_kwargs[\"load_in_8bit\"] = True\n",
        "                else:\n",
        "                    # Use 4-bit quantization for smaller GPUs\n",
        "                    load_kwargs[\"load_in_4bit\"] = True\n",
        "                    load_kwargs[\"bnb_4bit_compute_dtype\"] = torch.float16\n",
        "            except:\n",
        "                # Fallback if VRAM detection fails\n",
        "                load_kwargs[\"load_in_8bit\"] = True\n",
        "\n",
        "        # Load the model with determined settings\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
        "\n",
        "        # Verify model loads successfully\n",
        "        test_input = tokenizer(\"Hello\", return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            test_output = model.generate(test_input.input_ids, max_new_tokens=5)\n",
        "\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {e}\")\n",
        "        st.warning(\"Attempting to load a smaller model...\")\n",
        "\n",
        "        try:\n",
        "            # Last resort: tiny model that should work on almost any hardware\n",
        "            backup_model = \"EleutherAI/pythia-160m\"\n",
        "            tokenizer = AutoTokenizer.from_pretrained(backup_model)\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            model = AutoModelForCausalLM.from_pretrained(backup_model, device_map=\"auto\", low_cpu_mem_usage=True)\n",
        "            return model, tokenizer\n",
        "        except Exception as e2:\n",
        "            st.error(f\"Error loading backup model: {e2}\")\n",
        "            return None, None\n",
        "\n",
        "# Optimized text generation with streaming capability\n",
        "def generate_with_streaming(prompt, model, tokenizer, device, placeholder, speed_factor=1.0):\n",
        "    \"\"\"Generate text with streaming output\"\"\"\n",
        "    # Prepare model inputs\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Set generation parameters - balance quality and speed\n",
        "    gen_config = {\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.92,\n",
        "        \"repetition_penalty\": 1.2,\n",
        "        \"do_sample\": True\n",
        "    }\n",
        "\n",
        "    # For larger models with GPU, enable additional optimizations\n",
        "    if device == \"cuda\" and hasattr(model, \"config\"):\n",
        "        gen_config[\"use_cache\"] = True\n",
        "\n",
        "    # Generate text with optimizations\n",
        "    with torch.no_grad():\n",
        "        # Generate the complete response first\n",
        "        outputs = model.generate(inputs.input_ids, **gen_config)\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract the actual response part (after the prompt)\n",
        "        response_parts = full_response.split(prompt)\n",
        "        if len(response_parts) > 1:\n",
        "            response = response_parts[-1].strip()\n",
        "        else:\n",
        "            # Alternative extraction method\n",
        "            try:\n",
        "                if \"<|assistant|>\" in full_response:\n",
        "                    response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
        "                else:\n",
        "                    response = full_response.strip()\n",
        "            except:\n",
        "                response = full_response\n",
        "\n",
        "        # Stream the response with a typing effect\n",
        "        # Calculate delay based on speed factor (smaller = faster)\n",
        "        delay = 0.01 / speed_factor\n",
        "\n",
        "        # Stream with increasing chunks for natural feel\n",
        "        partial_response = \"\"\n",
        "        chunk_size = 5  # Start with small chunks\n",
        "\n",
        "        for i in range(0, len(response), chunk_size):\n",
        "            # Gradually increase chunk size for more natural streaming\n",
        "            if i > 100:\n",
        "                chunk_size = 20\n",
        "            elif i > 50:\n",
        "                chunk_size = 10\n",
        "\n",
        "            end_idx = min(i + chunk_size, len(response))\n",
        "            partial_response = response[:end_idx]\n",
        "            placeholder.markdown(partial_response + \"▌\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "            # Occasionally add a slightly longer pause for natural reading\n",
        "            if i % 50 == 0 and i > 0:\n",
        "                time.sleep(delay * 3)\n",
        "\n",
        "        # Show final response\n",
        "        placeholder.markdown(response)\n",
        "\n",
        "        return response\n",
        "\n",
        "# Predefined high-quality responses - expanded set for common legal questions\n",
        "predefined_responses = {\n",
        "    \"arraignment\": \"\"\"An arraignment is a critical early stage in the criminal justice process where a defendant is formally presented with charges. During an arraignment, several key events occur:\n",
        "\n",
        "First, the defendant appears before a judge in a courtroom setting. The judge presides over the proceedings and makes legal determinations. The charges against the defendant are formally read aloud, ensuring the defendant understands exactly what they are being accused of.\n",
        "\n",
        "Second, the defendant is informed of their constitutional rights, including the right to counsel, the right to a trial, and the right against self-incrimination. If the defendant cannot afford an attorney, the court will appoint one. At this stage, the defendant enters a plea - typically guilty, not guilty, or no contest (nolo contendere).\n",
        "\n",
        "Third, the judge makes decisions about bail or pretrial release conditions. The judge considers factors such as the severity of the alleged crime, the defendant's criminal history, community ties, and whether they pose a flight risk or danger to the community. Based on these factors, the judge may set bail, release the defendant on their own recognizance, or in some serious cases, order the defendant to be held without bail.\n",
        "\n",
        "The participants in an arraignment include the judge (who presides over the proceeding), the prosecutor (representing the state/government), the defense attorney (representing the defendant), the defendant themselves, and court personnel such as the clerk and bailiff. Unlike trials, arraignments typically do not involve juries.\"\"\",\n",
        "\n",
        "    \"bail\": \"\"\"Bail is a financial arrangement that allows a defendant to be released from custody while awaiting trial. The primary purpose of bail is to ensure the defendant returns for future court appearances while balancing their right to freedom before conviction.\n",
        "\n",
        "There are several forms of bail commonly used in the criminal justice system. Cash bail requires the defendant to pay the full amount to the court. Surety bonds involve a bail bondsman who charges a fee (typically 10-15% of the bail amount) and posts the full bond on behalf of the defendant. Property bonds use real estate as collateral. Some defendants may be released on their own recognizance without financial requirements if the judge determines they pose little flight risk.\n",
        "\n",
        "When determining bail, judges consider multiple factors: the severity of the alleged crime, the defendant's criminal history, ties to the community, financial resources, and whether they pose a danger to the public. The Eighth Amendment to the U.S. Constitution prohibits \"excessive bail,\" though courts have considerable discretion in setting amounts.\n",
        "\n",
        "Not every defendant is eligible for bail. Those charged with the most serious crimes (like capital murder) or who pose significant flight risks may be held without bail. Similarly, defendants who violate previous bail conditions may have their bail revoked.\"\"\",\n",
        "\n",
        "    \"evidence\": \"\"\"Evidence in criminal cases falls into several distinct categories, each with specific legal implications and handling requirements.\n",
        "\n",
        "Direct evidence directly proves a fact without requiring inference or presumption. Examples include eyewitness testimony, video recordings of the crime in progress, or a suspect's confession. This type of evidence has straightforward probative value but may still have reliability issues, particularly with eyewitness accounts which can be affected by perception limitations, memory distortion, or bias.\n",
        "\n",
        "Circumstantial evidence requires inference to connect it to a conclusion. For example, fingerprints at a crime scene don't directly prove someone committed a crime, but they prove the person was present. Despite common misconceptions, circumstantial evidence can be extremely powerful, especially when multiple pieces corroborate each other. Many cases are successfully prosecuted primarily on circumstantial evidence.\n",
        "\n",
        "Physical evidence encompasses tangible objects relevant to the crime, such as weapons, documents, biological samples, or trace evidence like fibers and soil. The collection, preservation, and analysis of physical evidence must follow strict protocols to maintain the \"chain of custody\" - the documented chronology of the evidence's handling and storage. Breaks in this chain can render evidence inadmissible in court.\n",
        "\n",
        "Testimonial evidence comes from witnesses' statements, either in sworn affidavits or court testimony. The reliability of this evidence depends on factors including the witness's credibility, opportunity to observe, memory, and potential biases.\n",
        "\n",
        "Digital evidence has become increasingly important in modern investigations. This includes data from computers, mobile devices, social media accounts, surveillance systems, and IoT devices. Digital forensics specialists must follow particular protocols to ensure this evidence is authentic, complete, and admissible.\"\"\"\n",
        "}\n",
        "\n",
        "# Enhanced response generation\n",
        "def get_ai_response(query, context=None, role=\"investigator\", stream=True, placeholder=None):\n",
        "    \"\"\"Generate AI responses with multiple options for quality/speed balance\"\"\"\n",
        "    # Get cached model, tokenizer and device\n",
        "    model = st.session_state.model\n",
        "    tokenizer = st.session_state.tokenizer\n",
        "    device = st.session_state.device\n",
        "\n",
        "    if not model or not tokenizer:\n",
        "        return \"Model is not loaded. Please wait for the model to load or check your hardware requirements.\"\n",
        "\n",
        "    # Check cache for this query (using hash for consistency)\n",
        "    query_hash = hashlib.md5((query + (context or \"\") + role).encode()).hexdigest()\n",
        "    cached_response = response_cache.get(query_hash)\n",
        "\n",
        "    if cached_response:\n",
        "        # If streaming, simulate typing for cached response\n",
        "        if stream and placeholder:\n",
        "            # Show cached marker for transparency (only in dev mode)\n",
        "            if st.session_state.get('dev_mode', False):\n",
        "                placeholder.caption(\"Using cached response\")\n",
        "\n",
        "            # Display with typing effect\n",
        "            partial = \"\"\n",
        "            for i in range(0, len(cached_response), 5):\n",
        "                partial = cached_response[:i+5]\n",
        "                placeholder.markdown(partial + \"▌\")\n",
        "                time.sleep(0.01)  # Very fast for cached responses\n",
        "            placeholder.markdown(cached_response)\n",
        "\n",
        "        return cached_response\n",
        "\n",
        "    # Check predefined responses for exact matches to common questions\n",
        "    for key, response in predefined_responses.items():\n",
        "        if key in query.lower():\n",
        "            if stream and placeholder:\n",
        "                # Simulate streaming for predefined responses\n",
        "                partial = \"\"\n",
        "                for i in range(0, len(response), 10):\n",
        "                    partial = response[:i+10]\n",
        "                    placeholder.markdown(partial + \"▌\")\n",
        "                    time.sleep(0.01)  # Fast for predefined responses\n",
        "                placeholder.markdown(response)\n",
        "            return response\n",
        "\n",
        "    try:\n",
        "        # Define role-specific prompt templates\n",
        "        role_contexts = {\n",
        "            \"investigator\": \"You are an expert criminal investigator with extensive knowledge in forensics, detective work, and criminal profiling. You provide detailed analyses of crime scenes and investigative techniques.\",\n",
        "            \"legal_expert\": \"You are a seasoned legal professional specializing in criminal law, legal procedures, and case precedents. You offer comprehensive legal advice and analysis of complex legal situations. You have a deep understanding of the roles of judges, prosecutors, defense attorneys, and the court process.\",\n",
        "            \"prevention_specialist\": \"You are a crime prevention expert with comprehensive knowledge of safety strategies, risk mitigation techniques, and security systems. You provide detailed preventative measures tailored to specific scenarios.\",\n",
        "            \"news_analyst\": \"You are a professional crime news analyst with insights into current criminal trends, societal impacts, and patterns in criminal behavior. You offer detailed analysis of crime reporting and media coverage.\"\n",
        "        }\n",
        "\n",
        "        # Create system prompt\n",
        "        system_prompt = role_contexts.get(role, role_contexts[\"investigator\"])\n",
        "\n",
        "        # Add context from previous conversation if available\n",
        "        if context:\n",
        "            context_text = f\"\\nHere is some context from our previous conversation: {context}\\n\"\n",
        "        else:\n",
        "            context_text = \"\"\n",
        "\n",
        "        # Format prompt with the model's expected format\n",
        "        if hasattr(tokenizer, \"apply_chat_template\") and callable(tokenizer.apply_chat_template):\n",
        "            # For models that support chat templates\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": f\"{context_text}\\n\\nPlease provide a comprehensive, factually accurate response to this question:\\n\\n{query}\"}\n",
        "            ]\n",
        "            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        else:\n",
        "            # Manual formatting for models without chat template support\n",
        "            prompt = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{context_text}\\nPlease provide a comprehensive, factually accurate response to this question:\\n\\n{query}\\n<|assistant|>\\n\"\n",
        "\n",
        "        # Generate response with streaming if requested\n",
        "        if stream and placeholder:\n",
        "            # Stream the response as it's generated\n",
        "            response = generate_with_streaming(\n",
        "                prompt,\n",
        "                model,\n",
        "                tokenizer,\n",
        "                device,\n",
        "                placeholder,\n",
        "                speed_factor=1.5  # Slightly faster than default\n",
        "            )\n",
        "        else:\n",
        "            # Generate without streaming for background tasks\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.92,\n",
        "                    repetition_penalty=1.1\n",
        "                )\n",
        "            full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract the response from the full output\n",
        "            response_parts = full_output.split(query)\n",
        "            if len(response_parts) > 1:\n",
        "                response = response_parts[-1].strip()\n",
        "            else:\n",
        "                # Alternative extraction method\n",
        "                if \"<|assistant|>\" in full_output:\n",
        "                    response = full_output.split(\"<|assistant|>\")[-1].strip()\n",
        "                else:\n",
        "                    response = full_output\n",
        "\n",
        "        # Format response into paragraphs if needed\n",
        "        if len(response) > 300 and \"\\n\\n\" not in response:\n",
        "            sentences = response.split('. ')\n",
        "            paragraphs = []\n",
        "            current_paragraph = []\n",
        "\n",
        "            for sentence in sentences:\n",
        "                if not sentence.endswith('.'):\n",
        "                    sentence += '.'\n",
        "                current_paragraph.append(sentence)\n",
        "\n",
        "                if len(current_paragraph) >= 3:  # Group ~3 sentences per paragraph\n",
        "                    paragraphs.append(' '.join(current_paragraph))\n",
        "                    current_paragraph = []\n",
        "\n",
        "            if current_paragraph:  # Add any remaining sentences\n",
        "                paragraphs.append(' '.join(current_paragraph))\n",
        "\n",
        "            response = '\\n\\n'.join(paragraphs)\n",
        "\n",
        "        # Cache the response for future use\n",
        "        response_cache[query_hash] = response\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error processing query: {str(e)}\"\n",
        "\n",
        "        if stream and placeholder:\n",
        "            placeholder.error(error_msg)\n",
        "\n",
        "        # Try fallback to a predefined response based on query keywords\n",
        "        if \"arraignment\" in query.lower():\n",
        "            return predefined_responses.get(\"arraignment\", \"An arraignment is the first formal court proceeding where the defendant hears the charges.\")\n",
        "        elif \"bail\" in query.lower():\n",
        "            return predefined_responses.get(\"bail\", \"Bail is a financial arrangement allowing defendants to be released from custody while awaiting trial.\")\n",
        "        elif \"evidence\" in query.lower():\n",
        "            return predefined_responses.get(\"evidence\", \"Evidence in criminal cases can be direct or circumstantial.\")\n",
        "        else:\n",
        "            return \"I apologize, but I couldn't process your question properly. Please try asking a simpler question or check your hardware resources.\"\n",
        "\n",
        "# Parallel document processing\n",
        "def process_document_in_parallel(text, chunk_size=5000):\n",
        "    \"\"\"Process document text in parallel chunks for faster analysis\"\"\"\n",
        "    # Split document into manageable chunks\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    # Process chunks in parallel\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        # Function to process each chunk\n",
        "        def analyze_chunk(chunk):\n",
        "            # Simple keyword extraction - can be enhanced with NLP\n",
        "            legal_terms = [\"court\", \"defendant\", \"plaintiff\", \"testimony\", \"evidence\",\n",
        "                         \"judge\", \"attorney\", \"ruling\", \"verdict\", \"motion\", \"objection\"]\n",
        "\n",
        "            found_terms = []\n",
        "            for term in legal_terms:\n",
        "                if term in chunk.lower():\n",
        "                    count = chunk.lower().count(term)\n",
        "                    found_terms.append(f\"{term} ({count})\")\n",
        "\n",
        "            return {\n",
        "                \"excerpt\": chunk[:100] + \"...\",\n",
        "                \"terms\": found_terms,\n",
        "                \"length\": len(chunk)\n",
        "            }\n",
        "\n",
        "        # Process all chunks\n",
        "        results = list(executor.map(analyze_chunk, chunks))\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_interactive_ai_tab(title, role, initial_context=None):\n",
        "    \"\"\"Create an interactive AI tab with conversation history\"\"\"\n",
        "    # Unique session state for each tab\n",
        "    session_key = f\"{role}_conversation_history\"\n",
        "\n",
        "    # Initialize conversation history\n",
        "    if session_key not in st.session_state:\n",
        "        st.session_state[session_key] = []\n",
        "        if initial_context:\n",
        "            st.session_state[session_key].append({\n",
        "                'role': 'system',\n",
        "                'content': initial_context\n",
        "            })\n",
        "\n",
        "    # Display conversation history\n",
        "    for message in st.session_state[session_key]:\n",
        "        if message['role'] == 'user':\n",
        "            st.chat_message(\"user\").write(message['content'])\n",
        "        elif message['role'] == 'assistant':\n",
        "            st.chat_message(\"assistant\").write(message['content'])\n",
        "        elif message['role'] == 'system':\n",
        "            st.info(message['content'])\n",
        "\n",
        "    # Chat input\n",
        "    user_query = st.chat_input(f\"Ask {title} a question...\")\n",
        "\n",
        "    if user_query:\n",
        "        # Check if model is loaded\n",
        "        if not st.session_state.model_loaded:\n",
        "            with st.spinner(\"Loading AI model (this may take a minute)...\"):\n",
        "                # Load model and prepare for use\n",
        "                device = check_gpu_availability()\n",
        "                model, tokenizer = load_advanced_model(device, st.session_state.advanced_mode)\n",
        "\n",
        "                if model and tokenizer:\n",
        "                    st.session_state.model = model\n",
        "                    st.session_state.tokenizer = tokenizer\n",
        "                    st.session_state.device = device\n",
        "                    st.session_state.model_loaded = True\n",
        "                    st.success(\"AI model loaded successfully!\")\n",
        "                else:\n",
        "                    st.error(\"Failed to load model. Please check your installation and hardware.\")\n",
        "                    return\n",
        "\n",
        "        # Add user message to history\n",
        "        st.session_state[session_key].append({\n",
        "            'role': 'user',\n",
        "            'content': user_query\n",
        "        })\n",
        "\n",
        "        # Display user query\n",
        "        st.chat_message(\"user\").write(user_query)\n",
        "\n",
        "        # Generate AI response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            # Create a placeholder for streaming response\n",
        "            response_placeholder = st.empty()\n",
        "\n",
        "            # Start timestamp for performance measurement\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Prepare context from previous messages (limited to last 3 for speed)\n",
        "            recent_messages = st.session_state[session_key][-4:]  # User's new message + up to 3 previous\n",
        "            context = \"\\n\".join([\n",
        "                f\"{msg['role']}: {msg['content']}\"\n",
        "                for msg in recent_messages\n",
        "                if msg['role'] != 'system'\n",
        "            ])\n",
        "\n",
        "            # Generate response with streaming\n",
        "            ai_response = get_ai_response(\n",
        "                user_query,\n",
        "                context=context,\n",
        "                role=role,\n",
        "                stream=True,\n",
        "                placeholder=response_placeholder\n",
        "            )\n",
        "\n",
        "            # Calculate response time\n",
        "            response_time = time.time() - start_time\n",
        "\n",
        "            # Show response time in subtle format\n",
        "            st.caption(f\"Response time: {response_time:.2f} seconds\")\n",
        "\n",
        "            # Store AI response in history\n",
        "            st.session_state[session_key].append({\n",
        "                'role': 'assistant',\n",
        "                'content': ai_response\n",
        "            })\n",
        "\n",
        "    # Advanced options\n",
        "    with st.expander(\"Conversation Controls\", expanded=False):\n",
        "        # Reset conversation button\n",
        "        if st.button(f\"🔄 Reset {title} Conversation\", key=f\"reset_conversation_{role}\"):\n",
        "            st.session_state[session_key] = []\n",
        "            if initial_context:\n",
        "                st.session_state[session_key].append({\n",
        "                    'role': 'system',\n",
        "                    'content': initial_context\n",
        "                })\n",
        "            st.rerun()\n",
        "\n",
        "        # Add copy button with unique key\n",
        "        if st.session_state[session_key] and len(st.session_state[session_key]) > 1:\n",
        "            if st.button(\"📋 Copy Last Response\", key=f\"copy_response_{role}\"):\n",
        "                last_response = next((msg['content'] for msg in reversed(st.session_state[session_key])\n",
        "                                    if msg['role'] == 'assistant'), None)\n",
        "                if last_response:\n",
        "                    st.success(\"Response copied to clipboard!\")\n",
        "\n",
        "def fetch_crime_news():\n",
        "    \"\"\"Fetch recent crime-related news articles with caching\"\"\"\n",
        "    # Use cache to avoid redundant API calls\n",
        "    @st.cache_data(ttl=3600)  # Cache for 1 hour\n",
        "    def cached_news_fetch():\n",
        "        params = {\n",
        "            'q': 'crime OR criminal OR police',\n",
        "            'apiKey': NEWS_API_KEY,\n",
        "            'language': 'en',\n",
        "            'pageSize': 5\n",
        "        }\n",
        "        try:\n",
        "            response = requests.get(BASE_URL, params=params)\n",
        "            data = response.json()\n",
        "            return data.get('articles', [])\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error fetching news: {e}\")\n",
        "            return []\n",
        "\n",
        "    return cached_news_fetch()\n",
        "\n",
        "def format_news_article(article):\n",
        "    \"\"\"Format news article with proper rendering and truncation\"\"\"\n",
        "    title = article.get('title', 'No Title')\n",
        "    source = article.get('source', {}).get('name', 'Unknown Source')\n",
        "    date = article.get('publishedAt', '')[:10] if article.get('publishedAt') else ''\n",
        "    description = article.get('description', 'No description available')\n",
        "    url = article.get('url', '')\n",
        "\n",
        "    # Handle potential rendering issues with descriptions\n",
        "    if description:\n",
        "        # Truncate long descriptions\n",
        "        if len(description) > 200:\n",
        "            description = description[:200] + \"...\"\n",
        "\n",
        "        # Fix potential HTML tags in description\n",
        "        description = description.replace('<', '&lt;').replace('>', '&gt;')\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'source': source,\n",
        "        'date': date,\n",
        "        'description': description,\n",
        "        'url': url\n",
        "    }\n",
        "\n",
        "# Create a dynamic crime dashboard\n",
        "def create_crime_dashboard():\n",
        "    \"\"\"Create an interactive crime statistics dashboard\"\"\"\n",
        "    st.subheader(\"Crime Hotspot Visualization\")\n",
        "\n",
        "    # Sample data - could be replaced with real data source\n",
        "    crime_data = pd.DataFrame({\n",
        "        \"City\": [\"Delhi\", \"Mumbai\", \"Kolkata\", \"Chennai\", \"Bangalore\",\n",
        "                \"Hyderabad\", \"Pune\", \"Ahmedabad\", \"Jaipur\", \"Lucknow\"],\n",
        "        \"Latitude\": [28.7041, 19.0760, 22.5726, 13.0827, 12.9716,\n",
        "                    17.3850, 18.5204, 23.0225, 26.9124, 26.8467],\n",
        "        \"Longitude\": [77.1025, 72.8777, 88.3639, 80.2707, 77.5946,\n",
        "                    78.4867, 73.8567, 72.5714, 75.7873, 80.9462],\n",
        "        \"Crime_Rate\": [780, 720, 640, 600, 580, 550, 520, 490, 470, 450],\n",
        "        \"Burglary\": [320, 280, 250, 220, 210, 190, 180, 170, 160, 150],\n",
        "        \"Assault\": [280, 260, 240, 230, 220, 210, 200, 190, 180, 170],\n",
        "        \"Vehicle_Theft\": [180, 170, 150, 140, 130, 120, 110, 100, 90, 80]\n",
        "    })\n",
        "\n",
        "    # Add tabs for different visualizations\n",
        "    map_tab, chart_tab, table_tab = st.tabs([\"Crime Map\", \"Crime Charts\", \"Data Table\"])\n",
        "\n",
        "    with map_tab:\n",
        "        # Create map options\n",
        "        crime_type = st.selectbox(\n",
        "            \"Select Crime Type\",\n",
        "            [\"Overall Crime Rate\", \"Burglary\", \"Assault\", \"Vehicle Theft\"],\n",
        "            key=\"map_crime_selector\"\n",
        "        )\n",
        "\n",
        "        # Map the selection to dataframe column\n",
        "        crime_column_map = {\n",
        "            \"Overall Crime Rate\": \"Crime_Rate\",\n",
        "            \"Burglary\": \"Burglary\",\n",
        "            \"Assault\": \"Assault\",\n",
        "            \"Vehicle Theft\": \"Vehicle_Theft\"\n",
        "        }\n",
        "\n",
        "        selected_column = crime_column_map[crime_type]\n",
        "\n",
        "        # Create interactive map with folium\n",
        "        m = folium.Map(location=[20.5937, 78.9629], zoom_start=5)\n",
        "\n",
        "        # Add crime markers\n",
        "        for _, row in crime_data.iterrows():\n",
        "            # Scale circle size proportionally\n",
        "            radius = row[selected_column] / 40  # Adjust divisor to get reasonable circle sizes\n",
        "\n",
        "            # Create color based on crime rate (red = high, yellow = medium, green = low)\n",
        "            max_value = crime_data[selected_column].max()\n",
        "            crime_ratio = row[selected_column] / max_value\n",
        "\n",
        "            if crime_ratio > 0.7:\n",
        "                color = \"red\"\n",
        "            elif crime_ratio > 0.4:\n",
        "                color = \"orange\"\n",
        "            else:\n",
        "                color = \"green\"\n",
        "\n",
        "            # Create popup content with HTML for better formatting\n",
        "            popup_html = f\"\"\"\n",
        "            <div style=\"width:200px\">\n",
        "                <h4>{row['City']}</h4>\n",
        "                <b>{crime_type}:</b> {row[selected_column]}<br>\n",
        "                <b>Overall Crime Rate:</b> {row['Crime_Rate']}\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "            # Add interactive marker\n",
        "            folium.CircleMarker(\n",
        "                location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "                radius=radius,\n",
        "                color=color,\n",
        "                fill=True,\n",
        "                fill_color=color,\n",
        "                fill_opacity=0.7,\n",
        "                popup=folium.Popup(popup_html, max_width=300)\n",
        "            ).add_to(m)\n",
        "\n",
        "        # Display the map\n",
        "        folium_static(m)\n",
        "\n",
        "    with chart_tab:\n",
        "        # Create crime comparison chart\n",
        "        st.subheader(\"Crime Type Comparison by City\")\n",
        "\n",
        "        # Melt the dataframe for plotting\n",
        "        plot_data = pd.melt(\n",
        "            crime_data,\n",
        "            id_vars=['City'],\n",
        "            value_vars=['Burglary', 'Assault', 'Vehicle_Theft'],\n",
        "            var_name='Crime Type',\n",
        "            value_name='Count'\n",
        "        )\n",
        "\n",
        "        # Create interactive bar chart\n",
        "        fig = px.bar(\n",
        "            plot_data,\n",
        "            x='City',\n",
        "            y='Count',\n",
        "            color='Crime Type',\n",
        "            barmode='group',\n",
        "            title=\"Crime Distribution by City\",\n",
        "            height=500\n",
        "        )\n",
        "\n",
        "        # Customize layout\n",
        "        fig.update_layout(\n",
        "            xaxis_title=\"City\",\n",
        "            yaxis_title=\"Number of Incidents\",\n",
        "            legend_title=\"Crime Type\",\n",
        "            font=dict(size=12)\n",
        "        )\n",
        "\n",
        "        # Display the chart\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "        # Add trend analysis\n",
        "        st.subheader(\"Crime Trend Analysis\")\n",
        "\n",
        "        # Create sample time series data\n",
        "        months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"]\n",
        "        trend_data = pd.DataFrame({\n",
        "            \"Month\": months * 3,\n",
        "            \"Crime Type\": [\"Burglary\"] * 6 + [\"Assault\"] * 6 + [\"Vehicle Theft\"] * 6,\n",
        "            \"Count\": [120, 115, 118, 125, 115, 110,\n",
        "                      80, 85, 90, 88, 95, 92,\n",
        "                      65, 60, 55, 58, 62, 60]\n",
        "        })\n",
        "\n",
        "        # Create line chart\n",
        "        trend_fig = px.line(\n",
        "            trend_data,\n",
        "            x=\"Month\",\n",
        "            y=\"Count\",\n",
        "            color=\"Crime Type\",\n",
        "            markers=True,\n",
        "            title=\"Monthly Crime Trends\"\n",
        "        )\n",
        "\n",
        "        # Display trend chart\n",
        "        st.plotly_chart(trend_fig, use_container_width=True)\n",
        "\n",
        "    with table_tab:\n",
        "        # Display sortable data table\n",
        "        st.subheader(\"Crime Statistics Data Table\")\n",
        "        st.dataframe(\n",
        "            crime_data,\n",
        "            hide_index=True,\n",
        "            column_config={\n",
        "                \"City\": st.column_config.TextColumn(\"City\"),\n",
        "                \"Crime_Rate\": st.column_config.NumberColumn(\"Overall Crime Rate\", format=\"%.0f\"),\n",
        "                \"Burglary\": st.column_config.NumberColumn(\"Burglary\", format=\"%.0f\"),\n",
        "                \"Assault\": st.column_config.NumberColumn(\"Assault\", format=\"%.0f\"),\n",
        "                \"Vehicle_Theft\": st.column_config.NumberColumn(\"Vehicle Theft\", format=\"%.0f\")\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Add download option\n",
        "        csv = crime_data.to_csv(index=False).encode('utf-8')\n",
        "        st.download_button(\n",
        "            \"Download Data as CSV\",\n",
        "            csv,\n",
        "            \"crime_statistics.csv\",\n",
        "            \"text/csv\",\n",
        "            key=\"download-csv\"\n",
        "        )\n",
        "\n",
        "# Main Streamlit App\n",
        "def main():\n",
        "    # Streamlit UI Setup\n",
        "    st.set_page_config(\n",
        "        page_title=\"Crime Investigation AI Pro\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\",\n",
        "        menu_items={\n",
        "            'About': \"Crime Investigation AI Pro - A comprehensive tool for criminal justice professionals\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Initialize session state\n",
        "    if 'advanced_mode' not in st.session_state:\n",
        "        st.session_state.advanced_mode = False\n",
        "\n",
        "    # Check GPU and determine device\n",
        "    device = check_gpu_availability()\n",
        "    st.session_state.device = device\n",
        "\n",
        "    # Sidebar for app settings\n",
        "    with st.sidebar:\n",
        "        st.title(\"🚔 AI Crime Console Pro\")\n",
        "        st.subheader(\"Application Settings\")\n",
        "\n",
        "        # App theme with functional implementation\n",
        "        theme = st.radio(\n",
        "            \"Interface Theme\",\n",
        "            [\"Professional\", \"Dark Mode\", \"High Contrast\"],\n",
        "            index=0,\n",
        "            key=\"theme_selection\"\n",
        "        )\n",
        "\n",
        "        # Apply selected theme using custom CSS\n",
        "        if theme == \"Dark Mode\":\n",
        "            st.markdown(\"\"\"\n",
        "            <style>\n",
        "                .stApp {background-color: #1E1E1E; color: #FFFFFF;}\n",
        "                .stTextInput>div>div>input {background-color: #333333; color: #FFFFFF;}\n",
        "                .stMarkdown {color: #FFFFFF;}\n",
        "                div[data-baseweb=\"card\"] {background-color: #333333;}\n",
        "                .css-145kmo2 {color: #FFFFFF !important;}\n",
        "            </style>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "        elif theme == \"High Contrast\":\n",
        "            st.markdown(\"\"\"\n",
        "            <style>\n",
        "                .stApp {background-color: #000000; color: #FFFFFF;}\n",
        "                .stTextInput>div>div>input {background-color: #000000; color: #FFFFFF; border: 2px solid #FFFF00;}\n",
        "                h1, h2, h3 {color: #FFFF00 !important;}\n",
        "                .stMarkdown {color: #FFFFFF; font-size: 1.1rem;}\n",
        "                div[data-baseweb=\"card\"] {background-color: #000000; border: 1px solid #FFFF00;}\n",
        "                .stButton>button {background-color: #FFFF00; color: #000000;}\n",
        "            </style>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Add advanced mode toggle\n",
        "        st.divider()\n",
        "        st.subheader(\"AI Model Settings\")\n",
        "        advanced_mode = st.toggle(\"Advanced AI Mode\", value=st.session_state.advanced_mode)\n",
        "\n",
        "        # Update session state if changed\n",
        "        if advanced_mode != st.session_state.advanced_mode:\n",
        "            st.session_state.advanced_mode = advanced_mode\n",
        "\n",
        "            # Reset model if already loaded\n",
        "            if st.session_state.model_loaded:\n",
        "                st.warning(\"AI mode changed. Model will reload with new settings.\")\n",
        "                st.session_state.model_loaded = False\n",
        "                st.session_state.model = None\n",
        "                st.session_state.tokenizer = None\n",
        "\n",
        "        # Show model status\n",
        "        if st.session_state.model_loaded:\n",
        "            st.success(\"✅ AI model loaded and running\")\n",
        "\n",
        "            # Add option to reload model\n",
        "            if st.button(\"Reload AI Model\"):\n",
        "                with st.spinner(\"Reloading model...\"):\n",
        "                    st.session_state.model_loaded = False\n",
        "                    st.session_state.model = None\n",
        "                    st.session_state.tokenizer = None\n",
        "\n",
        "                    model, tokenizer = load_advanced_model(device, st.session_state.advanced_mode)\n",
        "\n",
        "                    if model and tokenizer:\n",
        "                        st.session_state.model = model\n",
        "                        st.session_state.tokenizer = tokenizer\n",
        "                        st.session_state.model_loaded = True\n",
        "                        st.success(\"Model reloaded successfully!\")\n",
        "                    else:\n",
        "                        st.error(\"Failed to reload model.\")\n",
        "        else:\n",
        "            st.info(\"AI model will load when needed\")\n",
        "            if st.button(\"Load AI Model Now\"):\n",
        "                with st.spinner(\"Loading AI model...\"):\n",
        "                    model, tokenizer = load_advanced_model(device, st.session_state.advanced_mode)\n",
        "\n",
        "                    if model and tokenizer:\n",
        "                        st.session_state.model = model\n",
        "                        st.session_state.tokenizer = tokenizer\n",
        "                        st.session_state.model_loaded = True\n",
        "                        st.success(\"AI model loaded successfully!\")\n",
        "                    else:\n",
        "                        st.error(\"Failed to load model. Please check your installation and hardware.\")\n",
        "\n",
        "    # Main content\n",
        "    st.title(\"🚔 Crime Investigation AI Pro\")\n",
        "    st.subheader(\"AI-Powered Criminal Justice Assistant\")\n",
        "\n",
        "    # Create tabs\n",
        "    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([\n",
        "        \"🕵️‍♂️ Crime Solving AI\",\n",
        "        \"📑 Legal Document Analysis\",\n",
        "        \"📊 Crime Dashboard\",\n",
        "        \"📰 Real-Time Crime News\",\n",
        "        \"🛡️ Crime Prevention Strategies\",\n",
        "        \"⚖️ Criminal Law Consultation\"\n",
        "    ])\n",
        "\n",
        "    # Crime Solving AI Tab\n",
        "    with tab1:\n",
        "        st.subheader(\"🔍 Criminal Investigation Assistance\")\n",
        "        create_interactive_ai_tab(\n",
        "            \"Sherlock AI Detective\",\n",
        "            \"investigator\",\n",
        "            \"An expert system designed to assist in solving complex criminal cases.\"\n",
        "        )\n",
        "\n",
        "    # Legal Document Analysis Tab\n",
        "    with tab2:\n",
        "        st.subheader(\"📄 Legal Document AI Analysis\")\n",
        "\n",
        "        # File uploader\n",
        "        uploaded_file = st.file_uploader(\"📂 Upload Legal Document (PDF)\", type=[\"pdf\"])\n",
        "\n",
        "        if uploaded_file:\n",
        "            with st.spinner(\"Analyzing document...\"):\n",
        "                try:\n",
        "                    with pdfplumber.open(uploaded_file) as pdf:\n",
        "                        text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
        "\n",
        "                    # Process document in parallel for better performance\n",
        "                    analysis_results = process_document_in_parallel(text)\n",
        "\n",
        "                    # Create tabs for different analysis views\n",
        "                    doc_tabs = st.tabs([\"Document Summary\", \"Document Q&A\", \"Document Sections\"])\n",
        "\n",
        "                    # Document Summary Tab\n",
        "                    with doc_tabs[0]:\n",
        "                        # Display document metadata\n",
        "                        st.subheader(\"Document Overview\")\n",
        "                        col1, col2, col3 = st.columns(3)\n",
        "                        with col1:\n",
        "                            st.metric(\"Pages\", len(pdf.pages))\n",
        "                        with col2:\n",
        "                            st.metric(\"Words\", len(text.split()))\n",
        "                        with col3:\n",
        "                            st.metric(\"Characters\", len(text))\n",
        "\n",
        "                        # Extract and display key information\n",
        "                        st.subheader(\"Key Information\")\n",
        "\n",
        "                        # Extract potential dates from the document\n",
        "                        import re\n",
        "                        date_pattern = r'\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{2,4})\\b'\n",
        "                        dates = re.findall(date_pattern, text)\n",
        "\n",
        "                        # Extract potential case numbers or document IDs\n",
        "                        id_pattern = r'\\b(?:Case\\s+No\\.?\\s*|ID\\s*[:=#]\\s*|Document\\s+No\\.?\\s*|File\\s+No\\.?\\s*)([A-Z0-9-]+)\\b'\n",
        "                        case_ids = re.findall(id_pattern, text, re.IGNORECASE)\n",
        "\n",
        "                        # Display extracted information\n",
        "                        if dates:\n",
        "                            st.write(\"**Dates Mentioned:**\", \", \".join(dates[:5]))\n",
        "                            if len(dates) > 5:\n",
        "                                st.caption(f\"...and {len(dates) - 5} more dates\")\n",
        "\n",
        "                        if case_ids:\n",
        "                            st.write(\"**Potential Case/Document IDs:**\", \", \".join(case_ids[:3]))\n",
        "\n",
        "                        # Create a simple text summary\n",
        "                        st.subheader(\"Document Preview\")\n",
        "                        st.text_area(\"First 500 characters:\", text[:500] + \"...\", height=150)\n",
        "\n",
        "                    # Document Q&A Tab - Interactive queries about the document\n",
        "                    with doc_tabs[1]:\n",
        "                        st.subheader(\"Ask Questions About This Document\")\n",
        "\n",
        "                        # Initialize session state for document context\n",
        "                        if \"document_context\" not in st.session_state:\n",
        "                            st.session_state.document_context = text[:5000]  # Store first 5000 chars for context\n",
        "\n",
        "                        # Question input\n",
        "                        doc_question = st.text_input(\"Enter your question about the document:\")\n",
        "\n",
        "                        if doc_question:\n",
        "                            with st.spinner(\"Analyzing document and generating response...\"):\n",
        "                                # Create a response placeholder for streaming\n",
        "                                response_placeholder = st.empty()\n",
        "\n",
        "                                # Prepare context with document content\n",
        "                                document_context = f\"Document Title: {uploaded_file.name}\\n\\nDocument Content (excerpt): {text[:2000]}...\\n\\n\"\n",
        "\n",
        "                                # Generate AI response about the document\n",
        "                                ai_response = get_ai_response(\n",
        "                                    f\"Based on the document content provided, please answer this question: {doc_question}\",\n",
        "                                    context=document_context,\n",
        "                                    role=\"legal_expert\",\n",
        "                                    stream=True,\n",
        "                                    placeholder=response_placeholder\n",
        "                                )\n",
        "\n",
        "                        # Provide sample questions for guidance\n",
        "                        with st.expander(\"Sample Document Questions\", expanded=False):\n",
        "                            st.markdown(\"\"\"\n",
        "                            Here are some example questions you can ask about the document:\n",
        "\n",
        "                            - What are the main legal issues addressed in this document?\n",
        "                            - Summarize the key points of this legal document.\n",
        "                            - What parties are mentioned in this document?\n",
        "                            - What dates or deadlines are mentioned?\n",
        "                            - What legal terminology is used and what does it mean?\n",
        "                            - Are there any clear obligations or requirements specified?\n",
        "                            - What would be the next steps in this legal process?\n",
        "                            \"\"\")\n",
        "\n",
        "                    # Document Sections Tab\n",
        "                    with doc_tabs[2]:\n",
        "                        st.subheader(\"Document Sections\")\n",
        "                        for i, result in enumerate(analysis_results):\n",
        "                            with st.expander(f\"Section {i+1} - {result['excerpt']}\"):\n",
        "                                st.write(f\"Length: {result['length']} characters\")\n",
        "                                if result['terms']:\n",
        "                                    st.write(\"Legal terms found: \" + \", \".join(result['terms']))\n",
        "                                else:\n",
        "                                    st.write(\"No specific legal terms found in this section.\")\n",
        "\n",
        "                                # Add an option to view full section content\n",
        "                                if st.button(f\"View Full Section {i+1}\", key=f\"view_section_{i}\"):\n",
        "                                    start_idx = i * 5000\n",
        "                                    end_idx = start_idx + 5000\n",
        "                                    section_text = text[start_idx:end_idx] if start_idx < len(text) else \"Section not available\"\n",
        "                                    st.text_area(f\"Section {i+1} Content:\", section_text, height=200)\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error processing PDF: {e}\")\n",
        "                    st.info(\"Try uploading a different PDF file or check if the file is properly formatted.\")\n",
        "\n",
        "        else:\n",
        "            # Display information when no document is uploaded\n",
        "            st.info(\"\"\"\n",
        "            ## Document Analysis Instructions\n",
        "\n",
        "            Upload a legal document (PDF format) to analyze its content. Our AI will:\n",
        "\n",
        "            1. Extract key information\n",
        "            2. Allow you to ask specific questions about the document\n",
        "            3. Identify legal terminology and sections\n",
        "\n",
        "            After uploading, you can interact with the document through the analysis tabs.\n",
        "            \"\"\")\n",
        "\n",
        "            # Provide a sample document option\n",
        "            st.markdown(\"Don't have a document to analyze? Try using a sample legal document.\")\n",
        "            if st.button(\"Load Sample Document\"):\n",
        "                # This is a placeholder - in a real app, you would load a sample document\n",
        "                st.info(\"Sample document functionality would be implemented here.\")\n",
        "\n",
        "                # In a real implementation, you might do something like:\n",
        "                # sample_path = \"path/to/sample_legal_document.pdf\"\n",
        "                # with open(sample_path, \"rb\") as f:\n",
        "                #     st.session_state.sample_doc = f.read()\n",
        "                # st.experimental_rerun()\n",
        "\n",
        "    # Crime Dashboard Tab\n",
        "    with tab3:\n",
        "        create_crime_dashboard()\n",
        "\n",
        "    # Real-Time Crime News Tab\n",
        "    with tab4:\n",
        "        st.subheader(\"Latest Crime News\")\n",
        "\n",
        "        col1, col2 = st.columns([1, 1])\n",
        "\n",
        "        with col1:\n",
        "            # Add refresh button for news\n",
        "            refresh = st.button(\"🔄 Refresh News\")\n",
        "\n",
        "            # Fetch and display news\n",
        "            with st.spinner(\"Fetching latest crime news...\"):\n",
        "                news_articles = fetch_crime_news()\n",
        "\n",
        "            if news_articles:\n",
        "                for article in news_articles[:5]:\n",
        "                    formatted_article = format_news_article(article)\n",
        "                    with st.container():\n",
        "                        st.markdown(f\"#### {formatted_article['title']}\")\n",
        "                        st.caption(f\"Source: {formatted_article['source']} | {formatted_article['date']}\")\n",
        "                        st.markdown(f\"{formatted_article['description']}\")\n",
        "                        # Add Read More link\n",
        "                        if formatted_article['url']:\n",
        "                            st.markdown(f\"[Read more]({formatted_article['url']})\")\n",
        "                        st.divider()\n",
        "            else:\n",
        "                st.warning(\"No news articles found. Please check your internet connection or try again later.\")\n",
        "\n",
        "        with col2:\n",
        "            create_interactive_ai_tab(\n",
        "                \"Crime News Analyst\",\n",
        "                \"news_analyst\",\n",
        "                \"Providing insights and analysis on recent crime news and trends.\"\n",
        "            )\n",
        "\n",
        "    # Crime Prevention Strategies Tab\n",
        "    with tab5:\n",
        "        st.subheader(\"Crime Prevention and Safety Strategies\")\n",
        "\n",
        "        # Simplified single column layout\n",
        "        with st.container():\n",
        "            st.markdown(\"### Security Topics\")\n",
        "\n",
        "            # Topic selector for all security categories\n",
        "            security_topic = st.selectbox(\n",
        "                \"Select a security topic to explore\",\n",
        "                [\n",
        "                    \"Home Security Systems\",\n",
        "                    \"Personal Safety Tips\",\n",
        "                    \"Business Security\",\n",
        "                    \"Travel Security\",\n",
        "                    \"Digital Security\",\n",
        "                    \"Vehicle Security\",\n",
        "                    \"Community Safety Programs\"\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Display topic-specific content based on selection\n",
        "            topic_content = {\n",
        "                \"Home Security Systems\": \"\"\"\n",
        "                **Home Security Essentials:**\n",
        "\n",
        "                Modern security systems combine multiple layers of protection including:\n",
        "\n",
        "                1. Door/window sensors\n",
        "                2. Motion detectors\n",
        "                3. Security cameras\n",
        "                4. Smart locks\n",
        "                5. Alarm systems\n",
        "\n",
        "                Ask our AI for specific recommendations tailored to your needs.\n",
        "                \"\"\",\n",
        "\n",
        "                \"Business Security\": \"\"\"\n",
        "                **Business Security Framework:**\n",
        "\n",
        "                Comprehensive business security includes:\n",
        "\n",
        "                1. Risk assessment and security audits\n",
        "                2. Physical security measures\n",
        "                3. Employee training and protocols\n",
        "                4. Incident response planning\n",
        "                5. Regular security reviews\n",
        "\n",
        "                Ask our AI for detailed guidance on implementing these measures.\n",
        "                \"\"\",\n",
        "\n",
        "                \"Personal Safety Tips\": \"\"\"\n",
        "                **Personal Safety Fundamentals:**\n",
        "\n",
        "                1. Situational awareness training\n",
        "                2. Emergency response planning\n",
        "                3. Self-defense basics\n",
        "                4. Communication tools and techniques\n",
        "                5. Identifying and avoiding high-risk situations\n",
        "\n",
        "                Ask our AI for personalized safety recommendations.\n",
        "                \"\"\"\n",
        "            }\n",
        "\n",
        "            # Display the selected topic content or a default message\n",
        "            st.info(topic_content.get(security_topic, f\"Ask our AI expert about {security_topic} strategies and best practices.\"))\n",
        "\n",
        "            # Quick access buttons for common questions\n",
        "            st.markdown(\"### Quick Questions\")\n",
        "            quick_questions = st.columns(3)\n",
        "\n",
        "            with quick_questions[0]:\n",
        "                if st.button(\"Best security practices?\"):\n",
        "                    st.session_state[\"prevention_specialist_quick_q\"] = \"What are the best security practices for \" + security_topic.lower() + \"?\"\n",
        "\n",
        "            with quick_questions[1]:\n",
        "                if st.button(\"Cost-effective solutions?\"):\n",
        "                    st.session_state[\"prevention_specialist_quick_q\"] = \"What are the most cost-effective \" + security_topic.lower() + \" solutions?\"\n",
        "\n",
        "            with quick_questions[2]:\n",
        "                if st.button(\"Latest technologies?\"):\n",
        "                    st.session_state[\"prevention_specialist_quick_q\"] = \"What are the latest technologies for \" + security_topic.lower() + \"?\"\n",
        "\n",
        "        # Interactive AI assistant for prevention\n",
        "        create_interactive_ai_tab(\n",
        "            \"Prevention Specialist\",\n",
        "            \"prevention_specialist\",\n",
        "            \"Expert guidance on crime prevention, personal safety, and risk mitigation strategies.\"\n",
        "        )\n",
        "\n",
        "    # Criminal Law Consultation Tab\n",
        "    with tab6:\n",
        "        st.subheader(\"Criminal Law and Legal Consultation\")\n",
        "\n",
        "        # Simplified layout with expandable sections instead of nested tabs\n",
        "        st.markdown(\"### Criminal Justice Process\")\n",
        "\n",
        "        # Create a visual representation of the criminal justice process\n",
        "        stages = [\n",
        "            \"Investigation\", \"Arrest\", \"Arraignment\", \"Pre-Trial\",\n",
        "            \"Trial\", \"Sentencing\", \"Appeals\", \"Corrections\"\n",
        "        ]\n",
        "\n",
        "        # Create a horizontal display of the stages\n",
        "        cols = st.columns(len(stages))\n",
        "        for i, (col, stage) in enumerate(zip(cols, stages)):\n",
        "            with col:\n",
        "                st.markdown(f\"**{i+1}. {stage}**\")\n",
        "\n",
        "        # Add option to select stage for more information\n",
        "        selected_stage = st.selectbox(\"Select a stage to learn more:\", stages)\n",
        "\n",
        "        # Display information based on selected stage\n",
        "        stage_info = {\n",
        "            \"Investigation\": \"The phase where law enforcement collects evidence and determines if a crime occurred.\",\n",
        "            \"Arrest\": \"Taking a suspect into custody based on probable cause.\",\n",
        "            \"Arraignment\": \"The first court appearance where charges are formally presented.\",\n",
        "            \"Pre-Trial\": \"Period involving discovery, motions, and plea negotiations.\",\n",
        "            \"Trial\": \"The formal judicial examination of evidence to determine guilt.\",\n",
        "            \"Sentencing\": \"Determination of punishment for convicted defendants.\",\n",
        "            \"Appeals\": \"Process to request review of legal proceedings by a higher court.\",\n",
        "            \"Corrections\": \"Implementation of the sentence through incarceration, probation, or other means.\"\n",
        "        }\n",
        "\n",
        "        st.info(stage_info.get(selected_stage, \"Select a stage to see details\"))\n",
        "\n",
        "        # Constitutional rights in an expandable section\n",
        "        with st.expander(\"Constitutional Rights in Criminal Cases\", expanded=False):\n",
        "            st.markdown(\"\"\"\n",
        "            **Key Constitutional Protections:**\n",
        "\n",
        "            - **Fourth Amendment**: Protection against unreasonable searches and seizures\n",
        "            - **Fifth Amendment**: Right against self-incrimination and double jeopardy\n",
        "            - **Sixth Amendment**: Right to counsel, speedy trial, and to confront witnesses\n",
        "            - **Eighth Amendment**: Protection against excessive bail and cruel punishment\n",
        "            \"\"\")\n",
        "\n",
        "        # Common legal terms in an expandable section\n",
        "        with st.expander(\"Common Legal Terms\", expanded=False):\n",
        "            st.markdown(\"\"\"\n",
        "            - **Probable Cause**: Reasonable basis for believing a crime may have been committed\n",
        "            - **Reasonable Doubt**: Standard of proof required to validate a criminal conviction\n",
        "            - **Miranda Rights**: Procedural safeguards to protect a suspect's Fifth Amendment rights\n",
        "            - **Chain of Custody**: Chronological documentation showing the seizure, custody, control, and disposition of evidence\n",
        "            - **Plea Bargain**: Agreement where a defendant pleads guilty to a lesser charge in exchange for a more lenient sentence\n",
        "            \"\"\")\n",
        "\n",
        "        # Quick access buttons for common legal questions\n",
        "        st.markdown(\"### Quick Legal Questions\")\n",
        "        legal_quick_q = st.columns(2)\n",
        "\n",
        "        with legal_quick_q[0]:\n",
        "            if st.button(\"What happens at arraignment?\"):\n",
        "                st.session_state[\"legal_expert_quick_q\"] = \"What happens at an arraignment hearing?\"\n",
        "\n",
        "        with legal_quick_q[1]:\n",
        "            if st.button(\"Rights when arrested?\"):\n",
        "                st.session_state[\"legal_expert_quick_q\"] = \"What are my rights when being arrested?\"\n",
        "\n",
        "        # Interactive AI assistant for legal consultation\n",
        "        create_interactive_ai_tab(\n",
        "            \"Legal Counsel AI\",\n",
        "            \"legal_expert\",\n",
        "            \"Providing professional legal insights and guidance on criminal law matters.\"\n",
        "        )\n",
        "\n",
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtFjVWgLrzsP",
        "outputId": "a4564d79-c7b4-4758-e91e-f28ea96e362c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app3.py\n"
          ]
        }
      ]
    }
  ]
}